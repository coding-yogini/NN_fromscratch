{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IFT6135 Assignment 1\n",
    "\n",
    "Dataset: MNIST handwritten digit dataset\n",
    "\n",
    "Objective: Build a Neural Network from scratch, without a deep learning library such as TensorFlow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset \n",
    "#def loaddata(datapath=\"\"):\n",
    "train_data = np.loadtxt(\"mnist_train.csv\", delimiter=\",\")\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", delimiter=\",\")\n",
    " #   return (train_data, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables\n",
    "NumLabels=10\n",
    "ImageSize=28\n",
    "#NumFeatures="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and transform the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the values of the image data into interval [0.01, 0.99]\n",
    "\n",
    "fac = 255  *0.99 + 0.01\n",
    "train_images = np.asfarray(train_data[:, 1:]) / fac\n",
    "test_images = np.asfarray(test_data[:, 1:]) / fac\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(train_images.shape)\n",
    "print(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform the labels to onehot\n",
    "\n",
    "# lr = np.arrange(NumLabels)\n",
    "# train_labels_onehot=(lr==train_labels).astype(np.float)\n",
    "# test_labels_onehot=(lr==test_labels).astype(np.float)\n",
    "# # we don't want zeroes and ones in the labels neither:\n",
    "# train_labels_onehot[train_labels_onehot==0] = 0.01\n",
    "# train_labels_onehot[train_labels_onehot==1] = 0.99\n",
    "# test_labels_onehot[test_labels_onehot==0] = 0.01\n",
    "# test_labels_onehot[test_labels_onehot==1] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at one image\n",
    "img = train_images[0].reshape((28,28))\n",
    "plt.imshow(img, cmap=\"Greys\")\n",
    "plt.show()\n",
    "\n",
    "# # look at all the images\n",
    "# for i in range(NumLabels):\n",
    "#     img = train_imgs[i].reshape((28,28))\n",
    "#     plt.imshow(img, cmap=\"Greys\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train a neural network\n",
    "\n",
    "1. Design the Architecture\n",
    "    - 2 hidden layers\n",
    "    - output layer use softmax of 10 classes\n",
    "    - total num of parameters, incld bias, falls within range of [0.5M, 1.0M]\n",
    "    \n",
    "2. Compile the model\n",
    "    - select optimizer, loss function, metrics\n",
    "    - loss function: cross entropy - minimize to optimize the model parameters using stochastic gradient descent\n",
    "    \n",
    "3. Train the model \n",
    "    - train_images and train_labels\n",
    "    - epochs: 10 \n",
    "    - validation split\n",
    "\n",
    "4. Evaluate model\n",
    "    - on test_images and test_labels\n",
    "\n",
    "5. Predict \n",
    "    - on test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a class and definitions\n",
    "class myclass:\n",
    "    def __init__(self, x):\n",
    "        self.x=x\n",
    "    \n",
    "    def myfunc(self):\n",
    "        print(\"hello function \"+ str(self.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outside the class myclass\n",
    "def func2(y):\n",
    "    #print(self.x)\n",
    "    return 2*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "hello function 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize p1 as class\n",
    "p1=myclass(5)\n",
    "print(p1.x)\n",
    "p1.myfunc()\n",
    "\n",
    "# does not belong to class:\n",
    "func2(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NN(object):\n",
    "    \n",
    "    def __init__(self,hidden_dims=(1024,2048),n_hidden=2,mode='train',data=None,model_path=None):\n",
    "        self.hidden_dims=hidden_dims # hidden dimensions, default = (1024,2048)\n",
    "        self.n_hidden=n_hidden # hidden number of layers \n",
    "        self.mode=mode # train or predict\n",
    "        self.data=data # dataset, not 'data_path'\n",
    "        self.model_path=model_path # TODO: what is a model_path? \n",
    "        \n",
    "    # DEFINE NETWORK ARCHITECTURE\n",
    "        # num nodes in input layer - 28x28=784\n",
    "        nodes_input=np.dot(self.hidden_dims[0], self.hidden_dims[1])\n",
    "        nodes_hidden=500 # TODO: implement get_nodes_hidden(lower,upper, h1,h2)\n",
    "        nodes_output=10 # TODO\n",
    "        print(\"number of input nodes: \"+ str(nodes_input))\n",
    "        \n",
    "    # INITIALIZE THE PARAMETERS\n",
    "        self.params = self.initialize_params(nodes_input,nodes_hidden, nodes_output )\n",
    "        print(\"The W3 matrix shape is: \" + str(self.params[\"W3\"].shape))\n",
    "        print(\"The b3 vector shape is: \" + str(self.params[\"b3\"].shape))\n",
    "        \n",
    "    #if mode == 'train':\n",
    "        # train the model\n",
    "    #elif mode == 'test': \n",
    "        # predict \n",
    "    \n",
    "########################################   \n",
    "# Initialize the weight parameters, with one of 3 modes:\n",
    "# 'Zero' - all weights are zero\n",
    "# 'Normal' - sample from standard Normal distribution \n",
    "# 'Glorot' - sample from uniform distribution\n",
    "# returns weight matrix\n",
    "# TODO: manually encode, then loop through hidden layers \n",
    "########################################    \n",
    "   # def initialize_weights(self, n_hidden, dims, mode='zero'):\n",
    "    def initialize_params(self, nodes_input, nodes_hidden,nodes_output, mode='zero'):\n",
    "        if mode == 'zero':\n",
    "            print(\"initializing weights with zeros\")\n",
    "            W1 = np.zeros((nodes_hidden, nodes_input))\n",
    "            b1 = np.zeros((nodes_hidden,1))\n",
    "            W2 = np.zeros((nodes_hidden,nodes_hidden))\n",
    "            b2 = np.zeros((nodes_hidden,1)) \n",
    "            W3 = np.zeros((nodes_output,nodes_hidden))\n",
    "            b3 = np.zeros((nodes_output,1))\n",
    "            # create params dictionary              \n",
    "            params = {\"W1\": W1, \"b1\": b1,\n",
    "                       \"W2\": W2, \"b2\": b2,\n",
    "                       \"W3\": W3, \"b3\": b3}\n",
    "            return params\n",
    "            #loop through layer 1 and 2\n",
    "#             for i in range(1, n_hidden):\n",
    "#                 self.W[i] = np.zeros(dims[0])\n",
    "        elif mode == 'normal':\n",
    "            print(\"initializing weights with Normal distribution\")\n",
    "        elif mode == 'glorot':\n",
    "             print(\"initializing weights with Glorot distribution\")\n",
    "########################################\n",
    "# Activation functions\n",
    "########################################\n",
    "    # why do we need this ??\n",
    "    def activation(self,input):\n",
    "        return (sigmoid(input))\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        if (derivative == True):\n",
    "            return x*(1-x)\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def relu(self, M):\n",
    "        print (\"Apply Relu activation function\")\n",
    "        return np.where(M >= 0, M, 0)\n",
    "    \n",
    "    def softmax(self,x, derivative=False):\n",
    "        if (derivative==True):\n",
    "            return \n",
    "        return (np.exp(x) / float(sum(np.exp(x))))\n",
    "\n",
    "\n",
    "########################################  \n",
    "# Forward propagation on initialized params,\n",
    "# input - training/testing images\n",
    "# labels - training/testing labels TODO: why do we need labels??\n",
    "# then, during prediction/testing step on optimizised params\n",
    "# returns activations\n",
    "########################################  \n",
    "    def forward(self,input, labels, params):\n",
    "        print(\"forward prop\")\n",
    "        print(\"input shape is: \" + str(input.shape))\n",
    "        Z1 = np.dot(params[\"W1\"], input) + params[\"b1\"]\n",
    "        print(\"Z1 shape is: \" + str(Z1.shape)) #(500, 60000)\n",
    "        A1 = self.relu(Z1)\n",
    "        print(\"A1 shape is: \" + str(A1.shape))\n",
    "        Z2 = np.dot(params[\"W2\"], A1) + params[\"b2\"]\n",
    "        print(\"Z2 shape is: \" + str(Z2.shape))\n",
    "        A2 = self.relu(Z2)\n",
    "        print(\"A2 shape is: \" + str(A2.shape))\n",
    "        Z3 = np.dot(params[\"W3\"], A2) + params[\"b3\"]\n",
    "        print(\"Z3 shape is: \" + str(Z3.shape))\n",
    "        A3 = self.softmax(Z3)\n",
    "        print(\"A3 shape is: \" + str(A3.shape))\n",
    "\n",
    "        activatations = {\"Z1\": Z1, \"A1\": A1,\n",
    "                         \"Z2\": Z2, \"A2\": A2,\n",
    "                         \"Z3\": Z3, \"A3\": A3, \n",
    "        }\n",
    "        return activations\n",
    "    \n",
    "    \n",
    "######################################## \n",
    "# Update params with the gradients\n",
    "# returns optimized params\n",
    "######################################## \n",
    "def update(self, grads, params, learningRate):  \n",
    "    params[\"W1\"] = params[\"W1\"]-(learningRate*grads[\"dW1\"])\n",
    "    params[\"b1\"] = params[\"b1\"]-(learningRate*grads[\"db1\"])\n",
    "    params[\"W2\"] = params[\"W2\"]-(learningRate*grads[\"dW2\"])\n",
    "    params[\"b2\"] = params[\"b2\"]-(learningRate*grads[\"db2\"])\n",
    "    params[\"W3\"] = params[\"W3\"]-(learningRate*grads[\"dW3\"])\n",
    "    params[\"b3\"] = params[\"b3\"]-(learningRate*grads[\"db3\"])\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################                \n",
    "# Train the model\n",
    "# returns params\n",
    "########################################  \n",
    "    def train(self, input, labels, learningRate):\n",
    "        print(\"train the model\")\n",
    "        # forward propagation on training set, given the initialized params \n",
    "        self.activations = self.forward(input,labels,self.params)\n",
    "#         error = model.loss(results.A2, y)\n",
    "#         derivatives = model.backward()\n",
    "#         params =model.update(learningRate)\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of input nodes: 784\n",
      "initializing weights with zeros\n",
      "The W3 matrix shape is: (10, 500)\n",
      "The b3 vector shape is: (10, 1)\n",
      "train the model\n",
      "forward prop\n",
      "input shape is: (784, 60000)\n",
      "Z1 shape is: (500, 60000)\n",
      "Apply Relu activation function\n",
      "A1 shape is: (500, 60000)\n",
      "Z2 shape is: (500, 60000)\n",
      "Apply Relu activation function\n",
      "A2 shape is: (500, 60000)\n",
      "Z3 shape is: (10, 60000)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-aeb36e4960a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#record the average loss measured on the training data at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#of each epoch (10 values for each setup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#model = NN((28,28),2, 'test', )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-01fb9ff5ff5f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input, labels)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;31m# forward propagation on training set, given the initialized params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;31m#         error = model.loss(results.A2, y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m#         derivatives = model.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-01fb9ff5ff5f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, labels, params)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mZ3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Z3 shape is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mA3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A3 shape is: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-01fb9ff5ff5f>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(self, x, derivative)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mderivative\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# Define model:\n",
    "model = NN((28,28), 2, 'train')\n",
    "\n",
    "learningRate=1.2\n",
    "\n",
    "#record the average loss measured on the training data at the end \n",
    "#of each epoch (10 values for each setup)\n",
    "model.train(np.transpose(train_images), test_images, learningRate)\n",
    "\n",
    "#model = NN((28,28),2, 'test', )\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Iterate through the epochs\n",
    "#     for i in range (0, num_epochs):\n",
    "#         results = model.forward(x,y,params)\n",
    "#         error = model.loss(results.A2, y)\n",
    "#         derivatives = model.backward()\n",
    "#         params =model.update()\n",
    "   \n",
    "# Predict\n",
    "#predictions = NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## \n",
    "# Calculate error - cross entropy:\n",
    "# this error is minimized to optimize the model parameters using stochastic gradient descent\n",
    "# compare last result of prediction vs actual labels \n",
    "# prediction is the output from the last layer (num_input x num_classes)\n",
    "# actual is the labels (num_examples x 1), not a one-hot encoded vector\n",
    "# formula: sum(-log()) / m\n",
    "######################################## \n",
    "def loss(self, prediction, actual):\n",
    "    m = actual.shape[0]\n",
    "    print(m)\n",
    "    p = prediction # A3 -output of softmax act \n",
    "    # use array indexing to get softmax probability of the correct label for each sample\n",
    "    log_likelihood = -np.log(p[range(m),actual])\n",
    "    loss = -np.sum(log_likelihood)/m\n",
    "    return loss\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns derivatives \n",
    "def backward(self,params, activations, inputs, labels):\n",
    "    print(\"backward propagation\")\n",
    "    # get the number of examples\n",
    "    m = inputs.shape[1]\n",
    "    print(m)\n",
    "    \n",
    "    dZ3 = activations[\"A3\"] - labels\n",
    "    dW3\n",
    "    db3\n",
    "    dz2\n",
    "    dW2\n",
    "    db2\n",
    "    dz1\n",
    "    dz\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self):\n",
    "    print(\"test/predict the model\")\n",
    "    predictions = forward(inputs, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
